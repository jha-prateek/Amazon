{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np # Torch wrapper for Numpy\n\nimport os\nfrom PIL import Image\n\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom sklearn.preprocessing import MultiLabelBinarizer","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"97dcfadb41118534815a39441cd547be0271ee49","_cell_guid":"45d63034-a44c-47e8-7376-2deb00af03a9","trusted":true},"cell_type":"code","source":"IMG_PATH = '../input/train-jpg/'\nIMG_EXT = '.jpg'\nTRAIN_DATA = '../input/train_v2.csv'\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f961efd73fa939ea267f9cfde69991c05ba1e194","_cell_guid":"08a005ca-d963-5434-d60d-72d399cb7fe3","trusted":true},"cell_type":"code","source":"class KaggleAmazonDataset(Dataset):\n    \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n\n    Arguments:\n        A CSV file path\n        Path to image folder\n        Extension of images\n        PIL transforms\n    \"\"\"\n\n    def __init__(self, csv_path, img_path, img_ext, transform=None):\n    \n        tmp_df = pd.read_csv(csv_path)\n        assert tmp_df['image_name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n        \"Some images referenced in the CSV file were not found\"\n        \n        self.mlb = MultiLabelBinarizer()\n        self.img_path = img_path\n        self.img_ext = img_ext\n        self.transform = transform\n\n        self.X_train = tmp_df['image_name']\n        self.y_train = self.mlb.fit_transform(tmp_df['tags'].str.split()).astype(np.float32)\n\n    def __getitem__(self, index):\n        img = Image.open(self.img_path + self.X_train[index] + self.img_ext)\n        img = img.convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        label = torch.from_numpy(self.y_train[index])\n        return img, label\n\n    def __len__(self):\n        return len(self.X_train.index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cb1b0c274c7576f8316614fd7502a664434c4f8","_cell_guid":"98a20a0b-d39e-21a6-232b-990e916f6756","trusted":true},"cell_type":"code","source":"transformations = transforms.Compose([transforms.Scale(32), \n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n#                                       transforms.Normalize(mean=[-1,-1,-1],std=[2,2,2])\n                                     ])\n\ndataset = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,transformations)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data.sampler import SubsetRandomSampler\n\nbatch_size = 256\nvalidation_split = .2\nshuffle_dataset = True\nrandom_seed= 42\n\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\n\nif shuffle_dataset:\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\n\ntrain_indices, val_indices = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1119f5298649ecb89c2c5f852897c2f668c57daf","_cell_guid":"a2d57750-80fc-c8fe-9640-f276681f5549","trusted":true},"cell_type":"code","source":"train_loader = DataLoader(dataset,\n                          batch_size=256,\n                          sampler=train_sampler,\n                          num_workers=1, # 1 for CUDA\n                          pin_memory=True # CUDA only\n                         )\n\ntest_loader = DataLoader(dataset,\n                          batch_size=256,\n                          sampler=valid_sampler,\n                          num_workers=1, # 1 for CUDA\n                          pin_memory=True # CUDA only\n                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimage, label = next(iter(train_loader.dataset))\n\nplt.imshow(image[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AmazonModel(nn.Module):\n    def __init__(self, pretrained_model, in_features, out_features):\n        super(AmazonModel, self).__init__()\n        \n        self.pretrained_model = pretrained_model\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(in_features,out_features)\n        self.sigmoid = torch.nn.Sigmoid()\n    \n    def forward(self, x):\n        x = self.pretrained_model(x)\n        x = self.relu(x)\n        x = self.fc1(x)\n        x = self.sigmoid(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import models\n\nmodel = models.resnet18(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nnum_ftrs = model.fc.out_features\nno_label = len(train_loader.dataset.y_train[0])\n\nmodel.fc.weight.requires_grad = True\nmodel.fc.bias.requires_grad = True\n\nmodel = AmazonModel(model, num_ftrs, no_label)\nmodel = model.to(device)\n\ntorch.backends.cudnn.benchmark=True\n\n# for name, param in model.named_parameters():\n#     print(name, param.requires_grad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2e45e9e48ac8fa50d0f233af2e840c081060e19","_cell_guid":"7c18ddb7-cd5a-86d3-b3b9-4c6bc467e7ea","trusted":true},"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.003)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"637199af63cb44cf516ed56f5d3201d10720980e","_cell_guid":"745377b3-d942-a03a-76a9-e27cce51e01d","trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n#         data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.binary_cross_entropy(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset) - split,\n                100. * batch_idx / len(train_loader), loss.data.item()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e23cad6ee3132b1de0cf9f220e7c15181e4e2ea","_cell_guid":"5e7ff060-19da-1b01-28ce-bd2e72430fee","trusted":true},"cell_type":"code","source":"for epoch in range(2):\n    train(epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import fbeta_score, confusion_matrix\n\nf_scores = list()\n\ndef test(epoch):\n    model.eval()\n    for batch_idx, (data, target) in enumerate(test_loader):\n        data, target = data.to(device), target.to(device)\n#         data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.binary_cross_entropy(output, target)\n        if batch_idx % 10 == 0:\n            print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), split,\n                100. * batch_idx / len(test_loader), loss.data.item()))\n            f_scores.append([target, output])\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t, o in f_scores:\n    print(fbeta_score(t.cpu().detach().numpy(), np.where(o.cpu().detach().numpy() > 0.5, 1, 0), beta=2, average='samples'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import precision_score\n\n# for t, o in f_scores:\n#     print(precision_score(t.cpu().detach().numpy(), np.where(o.cpu().detach().numpy() > 0.5, 1, 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x = test_loader.dataset[40405][0].unsqueeze(0).to(device)\n# y = test_loader.dataset[40405][1]\n\n# model.eval()\n# y_ = model(x)\n# n = 6\n# print(y_, torch.topk(y_, n)[1][0])\n# print(y, torch.topk(y, n)[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (dataset.mlb.inverse_transform(np.where(y_.cpu().detach().numpy() > 0.5, 1, 0))[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nTEST_IMG_PATH = '../input/test-jpg-v2/'\nTEST_IMG_EXT = '.jpg'\nSUBMISSION_FILE = '../input/sample_submission_v2.csv'\nSUB_MAPPING = '../input/test_v2_file_mapping.csv'\n\nsub = pd.read_csv(SUBMISSION_FILE)\n# os.listdir('../working/')\nfor index in range(len(sub)):\n    img = Image.open(TEST_IMG_PATH + sub['image_name'][index] + TEST_IMG_EXT)\n    img = img.convert('RGB')\n    img = transformations(img)\n    Y_ = model(img.unsqueeze(0).to(device))\n    sub['tags'][index] = ' '.join(list(dataset.mlb.inverse_transform(np.where(Y_.cpu().detach().numpy() > 0.5, 1, 0))[0]))\n    if index % 10 == 0:\n        print('{} Files Completed!')\n            \n# sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission_resnet18.csv', index = False)\nos.listdir('../working/')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"980a38d3a688c235bae36e7d7382d3c5a675694d","_cell_guid":"2e306e2f-87f0-f753-ab41-841a3b097afa"},"cell_type":"markdown","source":"# Thank you for your attention\n\nHopefully that will help you get started. I still have a lot to figure out in PyTorch like:\n\n* Implementing the train / validation split\n* Figure out data augmentation (and not just random transformations or images)\n* Implementing early stopping\n* Automating computation of intermediate layers\n* Improving the display of each epochs\n\nIf you liked the kernel don't forget to vote and don't hesitate to comment."},{"metadata":{"_uuid":"048d866ee8581bddd981ecd9c68f201200a3c4eb","_cell_guid":"16f21935-088c-8590-9233-2700afeb3922"},"cell_type":"markdown","source":"## Full code\n\nI have published my full code of the competition in my [GitHub](https://github.com/mratsim/Amazon_Forest_Computer_Vision). (Note: I only worked on it in the first 2 weeks, so it probably lacks the latest findings)\n\nYou will find:\n  - [A script that output the mean and stddev of your image if you want to train from scratch](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/compute-mean-std.py#L28)\n\n  - [Using weighted loss function](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L61)\n\n  - [Logging your experiment](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L89)\n\n  - [Composing data augmentations](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L103), also [here](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p_data_augmentation.py#L181).\nNote use [Pillow-SIMD](https://python-pillow.org/pillow-perf/) instead of PIL/Pillow. It is even faster than OpenCV\n\n  - [Loading from a CSV that contains image path - 61 lines yeah](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p2_dataload.py#L23)\n\n  - [Equivalent in Keras - 216 lines ugh](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/k_dataloader.py). Note: so much lines were needed because by default in Keras you either have the data augmentation with ImageDataGenerator or lazy loading of images with \"flow_from_directory\" and there is no flow_from_csv\n\n  - [Model finetuning with custom PyCaffe weights](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p_neuro.py#L139)\n\n  - Train_test_split, [PyTorch version](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p_model_selection.py#L4) and [Keras version](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/k_model_selection.py#L4)\n\n- [Weighted sampling training so that the model view rare cases more often](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L131-L140)\n\n - [Custom Sampler creation, example for the balanced sampler](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p_sampler.py)\n\n - [Saving snapshots each epoch](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L171)\n\n - [Loading the best snapshot for prediction](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/pytorch_predict_only.py#L83)\n\n - [Failed word embeddings experiments](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/Embedding-RNN-Autoencoder.ipynb) to [combine image and text data](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/Dual_Feed_Image_Label.ipynb)\n\n - [Combined weighted loss function (softmax for unique weather tags, BCE for multilabel tags)](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p2_loss.py#L36)\n\n - [Selecting the best F2-threshold](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p2_metrics.py#L38) via stochastic search at the end of each epoch to [maximize validation score](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/526128239a6abcbb32fbf5b34ed8cc7a3cd87c4e/src/p2_validation.py#L49). This is then saved along model parameter.\n\n  - [CNN-RNN combination (work in progress)](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p3_neuroRNN.py#L10)"}],"metadata":{"_is_fork":false,"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","version":"3.6.1"},"_change_revision":0,"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":1}